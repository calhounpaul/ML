Repository: ML

File Structure:
ML-main/
    README.md
    .gitignore
    permathings/
        libs/
            secretary.py
            docker_tools.py
            selenium_tools.py
        prereqs/
            3_install_other.sh
            0_install_cuda_12.5.sh
            1_install_docker_engine.sh
            all.sh
            2_install_nvidia_container_toolkit.sh
        persistent_output_examples/
            meta_digest/
                ML_processed.txt
                claude_description_240906.md
        scripts/
            textgen_webui/
                build_docker.sh
                run.sh
            tts_webui/
                build_docker.sh
                run.sh
            utils/
                digest_git.py
                set_power.sh
                init_secrets.sh
            segmentation/
                segment-track-anything/
                    grab_weights.sh
                    Dockerfile
                    ... (1 files omitted)
                SAM/
                    Dockerfile
                    run.sh
                    workspace/
                        out.png
                        automask.py
                        ... (3 files omitted)
                OpenAdapt/
                    grab_weights.sh
                    Dockerfile
                    ... (1 files omitted)
                    workspace/
                        run.sh
                segment-caption-anything/
                    grab_weights.sh
                    Dockerfile
                    ... (1 files omitted)
                    workspace/
                        run.sh
            stable_diffusion_3/
                run.sh
            tformers_template/
                Dockerfile
                run.sh
            ollama/
                run_llava.sh
                screenshot.png.json
                describe_screenshot.sh
                run_web_bundle.sh
                test_simple_llava.sh
                ... (1 files omitted)
                example_output/
                    0825wallpaper-13_1280.jpg.json
                    0831wallpaper-11_1600.jpg.json
                    ... (73 files omitted)
            OpenDevin/
                run.sh
            LLaVA-NeXT/
                run_docker.sh
                Dockerfile
                workspace/
                    run.sh
            langchain/
                Dockerfile
                run.sh
            webscraping/
                run.py
            finetuning/
                simple_ft/
                    merge_run.sh
                    Dockerfile.12.1
                    ... (4 files omitted)
                    workdir/
                        finetune_swift.py
                        finetune_qlora_swift.py
                        ... (4 files omitted)
            ebook_processing/
                run_docker.sh
                compile_synthetic_data.py
                Dockerfile
                convert_to_text.sh
                run_docker2.sh
                ... (1 files omitted)
                workspace/
                    verify_swiftian.py
                    reduced_lines.txt
                    ... (6 files omitted)

File Contents:

-----new_file-----
 /.gitignore:
ephemera/*
*.pyc
*.pyo

-----new_file-----
 /README.md:
# ML

This repository is a comprehensive toolkit for advanced machine learning and AI tasks, focusing on natural language processing, image processing, and model fine-tuning. It provides a well-organized structure of scripts, tools, and configurations to support various aspects of machine learning workflows.

## Key Features:

- **Environment Setup**: Scripts for setting up CUDA, Docker, and other prerequisites.
- **Model Implementations**: Support for text generation (GPT-like models), image processing (Stable Diffusion, Segment Anything Model), and more.
- **Fine-tuning Tools**: Scripts and configurations for fine-tuning language models.
- **Containerization**: Extensive use of Docker for reproducible and portable ML environments.
- **Integration**: Works with popular platforms like Hugging Face and frameworks such as PyTorch and Transformers.
- **Full Pipeline Support**: Includes tools for data processing, model training, and inference.

This repository serves as a toolbox for AI and ML practitioners, offering resources for a wide range of deep learning and natural language processing tasks. Whether you're working on text generation, image segmentation, or custom model training, you'll find useful components here to accelerate your workflow.


-----new_file-----
 permathings: Empty folder or all files filtered

-----new_file-----
 /permathings/libs/docker_tools.py:
import os, sys, json, re, time, datetime, random, string, docker, atexit

this_modules_path = os.path.dirname(os.path.realpath(__file__))
parent_dir = os.path.dirname(this_modules_path)
sys.path.append(parent_dir)

def run_docker_container(image_name, port_bindings, environment_vars, volumes, detach=True):
    client = docker.from_env()
    container = client.containers.run(image_name, detach=detach, ports=port_bindings, environment=environment_vars, volumes=volumes)
    return container

def create_docker_image_from_dockerfile(dockerfile_path, image_name, tag="latest"):
    client = docker.from_env()
    image, build_log = client.images.build(path=dockerfile_path, tag=f"{image_name}:{tag}")
    return image, build_log

-----new_file-----
 /permathings/libs/secretary.py:
import os, sys, json, hashlib, getpass, subprocess

libs_dir = this_dir = os.path.dirname(os.path.realpath(__file__))
sys.path.append(libs_dir)
persistent_dir = os.path.dirname(libs_dir)
root_dir = os.path.dirname(persistent_dir)
ephemeral_dir = os.path.join(root_dir, 'ephemera')
secrets_file_path = os.path.join(ephemeral_dir, 'secrets.json')

DEFAULT_SECRETS = [
    'HF_TOKEN',
]

def get_current_hwid():
    hwid = None
    try:
        hwid = os.popen('hwid').read().strip().split(': ')[1]
    except Exception as e:
        print(f"Error getting HWID: {e}")
    return hwid

def get_hwid_sha3():
    HWID_SHA3 = hashlib.sha3_256(get_current_hwid().encode()).hexdigest()
    return HWID_SHA3

def get_secrets_if_exist():
    if not os.path.exists(secrets_file_path):
        return None
    else:
        with open(secrets_file_path, 'r') as f:
            secrets = json.load(f)
        return secrets
    
def validate_hwid():
    secrets = get_secrets_if_exist()
    if secrets is None:
    ...[truncated middle 1120 characters]...f2', shell=True).decode().strip()
        return decrypted_string
    except subprocess.CalledProcessError as e:
        print(f"Error during decryption: {e}")
        return None

def set_secret(secret_name, secret_value):
    if validate_hwid():
        secrets_data = get_secrets_if_exist()
    else:
        secrets_data = re_init_secrets_file()
    secrets_data[secret_name] = encrypt_with_hwid(secret_value)
    with open(secrets_file_path, 'w') as f:
        json.dump(secrets_data, f)
    return True

def get_secret(secret_name):
    if validate_hwid():
        secrets_data = get_secrets_if_exist()
    else:
        secrets_data = re_init_secrets_file()
    secret = secrets_data.get(secret_name)
    if secret is None:
        secret = getpass.getpass(f"Enter {secret_name}: ")
        set_secret(secret_name, secret)
        return secret
    else:
        return decrypt_with_hwid(secret)

def get_all_defaults():
    for secret_name in DEFAULT_SECRETS:
        get_secret(secret_name)


-----new_file-----
 /permathings/libs/selenium_tools.py:
import os, sys, json, re, time, datetime, random, string, docker, atexit
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.firefox.options import Options
from selenium.webdriver.common.desired_capabilities import DesiredCapabilities
from selenium.webdriver.common.by import By
from bs4 import BeautifulSoup
from io import BytesIO
import tempfile
import html2text

this_modules_path = os.path.dirname(os.path.realpath(__file__))
parent_dir = os.path.dirname(this_modules_path)
sys.path.append(parent_dir)

from docker_tools import run_docker_container

def init_selenium_docker(driver_port=4444, vnc_port=7900):
    container = run_docker_container("selenium/standalone-firefox", {f"{driver_port}/tcp": driver_port, f"{vnc_port}/tcp": vnc_port}, {}, {}, detach=True)
    return container

def stop_and_remove_selenium_docker(container):
    container.stop()
    container.remove()

def init_selenium_driver_inside_docker(driver_port=4444):
    options = Options()
    driver = webdriver.Remote(command_executor=f"http://localhost:{driver_port}/wd/hub", options=options)
    return driver

def test_tools():
    container = init_selenium_docker()
    time.sleep(3)
    driver = init_selenium_driver_inside_docker()
    driver.get("https://yahoo.com")
    time.sleep(5)
    #print page text
    source = driver.page_source
    #soup = BeautifulSoup(source, "html.parser")
    #text = soup.get_text()
    text = html2text.html2text(source)
    print(text)
    stop_and_remove_selenium_docker(container)
    print("Test done")


-----new_file-----
 permathings/persistent_output_examples: Empty folder or all files filtered

-----new_file-----
 /permathings/persistent_output_examples/meta_digest/ML_processed.txt:
Repository: ML

File Structure:
ML-main/
    .gitignore
    README.md
    permathings/
        prereqs/
            0_install_cuda_12.5.sh
            1_install_docker_engine.sh
            3_install_other.sh
            2_install_nvidia_container_toolkit.sh
            all.sh
        libs/
            selenium_tools.py
            docker_tools.py
            secretary.py
        scripts/
            OpenDevin/
                run.sh
            textgen_webui/
                build_docker.sh
                run.sh
            stable_diffusion_3/
                run.sh
            tformers_template/
                Dockerfile
                run.sh
            segmentation/
                SAM/
                    Dockerfile
                    run.sh
                    workspace/
                        outputs.txt
                        screenshot.png
                        out.png
                        test1.py
                        automask.py
                segment-caption-...[truncated middle 57029 characters]...
config = AutoConfig.from_pretrained(model_id)

num_added_tokens = tokenizer.add_tokens(NEW_TOKENS)

model = AutoModelForCausalLM.from_pretrained(
    model_id,
    quantization_config=bnb_config,
    low_cpu_mem_usage=True,
    return_dict=True,
    torch_dtype=torch.bfloat16,
    device_map=DEVICE_MAP,
    #device_map=device_map,
)
model.config.use_cache = False
model.config.pretraining_tp = 1

tokenizer.add_special_tokens({'pad_token': '</s>'})
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right"

model.resize_token_embeddings(len(tokenizer))

model = prepare_model_for_kbit_training(model)

#model = get_peft_model(model, peft_config)

trainer = SFTTrainer(
    model=model,
    train_dataset=full_dataset,
    peft_config=peft_config,
    dataset_text_field="text",
    max_seq_length=MAX_SEQ_LENGTH,
    tokenizer=tokenizer,
    args=training_arguments,
    packing=PACKING,
)

trainer.train()

trainer.model.save_pretrained(os.path.join(OUTPUT_DIR, "final_model"))


-----new_file-----
 /permathings/persistent_output_examples/meta_digest/claude_description_240906.md:
# ML Repository

This repository is a comprehensive toolkit for advanced machine learning and AI tasks, focusing on natural language processing, image processing, and model fine-tuning. It provides a well-organized structure of scripts, tools, and configurations to support various aspects of machine learning workflows.

## Key Features:

- **Environment Setup**: Scripts for setting up CUDA, Docker, and other prerequisites.
- **Model Implementations**: Support for text generation (GPT-like models), image processing (Stable Diffusion, Segment Anything Model), and more.
- **Fine-tuning Tools**: Scripts and configurations for fine-tuning language models.
- **Containerization**: Extensive use of Docker for reproducible and portable ML environments.
- **Integration**: Works with popular platforms like Hugging Face and frameworks such as PyTorch and Transformers.
- **Full Pipeline Support**: Includes tools for data processing, model training, and inference.

This repository serves as a toolbox for AI and ML practitioners, offering resources for a wide range of deep learning and natural language processing tasks. Whether you're working on text generation, image segmentation, or custom model training, you'll find useful components here to accelerate your workflow.


-----new_file-----
 /permathings/prereqs/0_install_cuda_12.5.sh:
#!/bin/bash

export DEBIAN_FRONTEND=noninteractive
wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-keyring_1.1-1_all.deb
sudo dpkg -i cuda-keyring_1.1-1_all.deb
sudo apt-get update
sudo apt-get -y install cuda-toolkit-12-5
sudo apt-get install -y cuda-drivers
sudo rm cuda-keyring_1.1-1_all.deb


-----new_file-----
 /permathings/prereqs/1_install_docker_engine.sh:
#!/bin/bash

for pkg in docker.io docker-doc docker-compose docker-compose-v2 podman-docker containerd runc; do sudo apt-get remove $pkg; done

# Add Docker's official GPG key:
sudo apt-get update
sudo apt-get install ca-certificates curl
sudo install -m 0755 -d /etc/apt/keyrings
sudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc
sudo chmod a+r /etc/apt/keyrings/docker.asc

# Add the repository to Apt sources:
echo \
  "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu \
  $(. /etc/os-release && echo "$VERSION_CODENAME") stable" | \
  sudo tee /etc/apt/sources.list.d/docker.list > /dev/null
sudo apt-get update

sudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin -y

sudo groupadd docker

sudo usermod -aG docker $USER

-----new_file-----
 /permathings/prereqs/2_install_nvidia_container_toolkit.sh:
#!/bin/bash

curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg \
  && curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | \
    sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \
    sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list

sudo apt-get update

sudo apt-get install -y nvidia-container-toolkit

sudo nvidia-ctk runtime configure --runtime=docker

sudo systemctl restart docker

-----new_file-----
 /permathings/prereqs/3_install_other.sh:
#!/bin/bash

sudo apt update
sudo apt install -y python3-pip python3-venv python3-dev jq imagemagick git-lfs

pip3 install --upgrade pip

pip3 install --upgrade docker selenium pillow hwid aes-cipher
sudo apt install -y calibre jq

sudo sed -i 's/<policy domain="coder" rights="none" pattern="PDF" \/>//g' /etc/ImageMagick-6/policy.xml


-----new_file-----
 /permathings/prereqs/all.sh:
bash ./0_*.sh
bash ./1_*.sh
bash ./2_*.sh
bash ./3_*.sh

-----new_file-----
 permathings/scripts: Empty folder or all files filtered

-----new_file-----
 /permathings/scripts/LLaVA-NeXT/Dockerfile:
FROM nvidia/cuda:12.5.0-devel-ubuntu22.04

ENV DEBIAN_FRONTEND=noninteractive

RUN apt update
RUN apt install -y git
RUN apt install -y curl
RUN apt install -y wget
RUN apt install -y python3.10-full
RUN apt install -y python3-pip python3-dev python3-venv
RUN python3 -m pip install --no-cache-dir --upgrade pip
RUN apt install -y tesseract-ocr libtesseract-dev

WORKDIR /app
RUN curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.35.3/install.sh | bash
RUN git clone https://github.com/OpenAdaptAI/OpenAdapt.git
WORKDIR /app/OpenAdapt
ENV PYTHONPATH "${PYTHONPATH}:/app/OpenAdapt"
RUN python3 -m pip install --no-cache-dir poetry
RUN poetry install
RUN poetry run install-dashboard
RUN python3 -m pip install --no-cache-dir .[all]
WORKDIR /app/OpenAdapt/openadapt
RUN alembic upgrade head
RUN pytest

-----new_file-----
 /permathings/scripts/LLaVA-NeXT/run_docker.sh:
THIS_DOCKER_CONTAINER_NAME="transformers-llava-next"

THIS_DIR_PATH=$(dirname $(realpath $0))
SCRIPTS_DIR_PATH=$(dirname $(dirname $THIS_DIR_PATH))
TEMPLATE_DIR_PATH=$SCRIPTS_DIR_PATH/tformers_template
TEMPLATE_IMAGE_NAME="transformers-quantization-latest-gpu"
PERMATHINGS_DIR_PATH=$(dirname $SCRIPTS_DIR_PATH)
ML_PROJECT_DIR_PATH=$(dirname $PERMATHINGS_DIR_PATH)
EPHEMERA_DIR_PATH=$ML_PROJECT_DIR_PATH/ephemera
SHARED_CACHES_DIR_PATH=$EPHEMERA_DIR_PATH/shared_caches

#docker build -t $TEMPLATE_IMAGE_NAME $TEMPLATE_DIR_PATH
docker build -t $THIS_DOCKER_CONTAINER_NAME .

if [ ! -d $SHARED_CACHES_DIR_PATH ]; then
    mkdir -p $SHARED_CACHES_DIR_PATH
fi

THIS_CACHE=$SHARED_CACHES_DIR_PATH/$THIS_DOCKER_CONTAINER_NAME

if [ ! -d $THIS_CACHE ]; then
    mkdir -p $THIS_CACHE
fi

WORKDIR_PATH=$THIS_DIR_PATH/workspace

if [ ! -d $WORKDIR_PATH ]; then
    mkdir -p $WORKDIR_PATH
fi

docker kill $THIS_DOCKER_CONTAINER_NAME
docker rm $THIS_DOCKER_CONTAINER_NAME
sleep 2
docker kill $THIS_DOCKER_CONTAINER_NAME
docker rm $THIS_DOCKER_CONTAINER_NAME
sleep 2

docker run --gpus all -it --rm \
    --name $THIS_DOCKER_CONTAINER_NAME \
    -v $THIS_CACHE:/root/.cache \
    -v $WORKDIR_PATH:/workspace \
    -w /workspace \
    -p 7860:7860 \
    $THIS_DOCKER_CONTAINER_NAME


-----new_file-----
 /permathings/scripts/LLaVA-NeXT/workspace/run.sh:


-----new_file-----
 /permathings/scripts/OpenDevin/run.sh:
OPENDEVIN_WORKSPACE=$(pwd)/workspace
export WORKSPACE_BASE=$(pwd)/workspace
docker run -it --rm \
    --pull=always \
    -e SANDBOX_USER_ID=$(id -u) \
    -e PERSIST_SANDBOX="true" \
    -e SSH_PASSWORD="temp_pass" \
    -e WORKSPACE_MOUNT_PATH=$OPENDEVIN_WORKSPACE \
    -v $OPENDEVIN_WORKSPACE:/opt/workspace_base \
    -v /var/run/docker.sock:/var/run/docker.sock \
    -p 3000:3000 \
    --add-host host.docker.internal:host-gateway \
    -e LLM_API_KEY="ollama" \
    -e LLM_BASE_URL="http://host.docker.internal:11434" \
    --name opendevin-app-$(date +%Y%m%d%H%M%S) \
    ghcr.io/opendevin/opendevin:0.6

#command-r-plus:104b-q2_K

-----new_file-----
 /permathings/scripts/ebook_processing/Dockerfile:
FROM nvidia/cuda:12.5.0-devel-ubuntu22.04

ENV DEBIAN_FRONTEND=noninteractive

RUN apt update
RUN apt install -y git
RUN apt install -y curl
RUN apt install -y wget
RUN apt install -y python3.10-full
RUN apt install -y python3-pip python3-dev python3-venv
RUN python3 -m pip install --no-cache-dir --upgrade pip
RUN python3 -m pip install --no-cache-dir --upgrade mistral-inference
RUN python3 -m pip install --no-cache-dir transformers huggingface_hub
RUN python3 -m pip install --no-cache-dir outlines
RUN python3 -m pip install --no-cache-dir pydantic
RUN python3 -m pip install --no-cache-dir protobuf
RUN python3 -m pip install --no-cache-dir accelerate
RUN python3 -m pip install --no-cache-dir bitsandbytes
RUN python3 -m pip install --no-cache-dir sentence-splitter
RUN python3 -m pip install --no-cache-dir Unidecode
WORKDIR /workspace

-----new_file-----
 permathings/scripts/ebook_processing: 1 file(s) skipped in this folder

-----new_file-----
 /permathings/scripts/ebook_processing/compile_synthetic_data.py:
import os, sys, json, random, string, re, time
import itertools

this_dir = os.path.dirname(os.path.abspath(__file__))
scripts_dir = os.path.dirname(this_dir)
permathings_dir = os.path.dirname(scripts_dir)
root_dir = os.path.dirname(permathings_dir)
ephemeral_dir = os.path.join(root_dir, "ephemera")
outputs_dir = os.path.join(ephemeral_dir, "outputs")
ebook_analyses_dir = os.path.join(outputs_dir, "ebook_analyses")
swift_quotes_dir = os.path.join(ebook_analyses_dir, "swift_quotes")

REPLACEMENTS = {
    "\u2019": "'",
    "\u2014": "-",
    "\u201c": "\"",
    "\u201d": "\"",
    "\u2026": "...",
    "\u2013": "-",
    "\u00a0": " ",
}

SKIP_STRINGS=[
    "_",
    "normalized",
    "\\n",
    "Z - nds",
    "G -- ",
    "d - l",
    "This quote",
    "----",
    "si- lent",
]

SKIP_IF_NOT_EQUALLY_IN_BOTH = [
    "---",
    "[",
    "]",
    "...",
    "#",
    ".--"
]

REMOVE_IF_STARTS_WITH = [
    ", ",
    "-- ",
    ": ",
    ". ",
]

all_chunks_data = {}
all_sentence_pairs = {}
for...[truncated middle 6076 characters]...  input()
        continue
    start_from_sentence = max(1,len(sentences_swift)-10)
    for i in range(start_from_sentence, len(sentences_swift)):
        this_sentence = sentences_plain[i]
        this_context = sentences_plain[:i]
        converted_to = sentences_swift[i]
        if this_sentence == converted_to:
            continue
        if len(this_sentence) < 10 or len(converted_to) < 10:
            continue
        this_line = SPECIAL_TOKENS["CONTEXT"] + " " + " ".join(this_context) + " " + SPECIAL_TOKENS["TO_CONVERT"] + " " + this_sentence + " " + SPECIAL_TOKENS["CONVERTED"] + " " + converted_to
        all_lines.append(this_line)

for datum in all_lines:
    sample_of_other_lines = [datum,] + random.sample(all_lines, 10)
    full_string = (" ").join(sample_of_other_lines)
    final_dataset.append(full_string)

random.shuffle(final_dataset)
#save the dataset
with open(os.path.join(ebook_analyses_dir, "final_dataset.json"), "w") as f:
    json.dump(final_dataset, f, indent=4)

-----new_file-----
 /permathings/scripts/ebook_processing/compile_synthetic_data_original.py:
import os, sys, json, random, string, re, time

this_dir = os.path.dirname(os.path.abspath(__file__))
scripts_dir = os.path.dirname(this_dir)
permathings_dir = os.path.dirname(scripts_dir)
root_dir = os.path.dirname(permathings_dir)
ephemeral_dir = os.path.join(root_dir, "ephemera")
outputs_dir = os.path.join(ephemeral_dir, "outputs")
ebook_analyses_dir = os.path.join(outputs_dir, "ebook_analyses")
swift_quotes_dir = os.path.join(ebook_analyses_dir, "swift_quotes")

REPLACEMENTS = {
    "\u2019": "'",
    "\u2014": "-",
    "\u201c": "\"",
    "\u201d": "\"",
    "\u2026": "...",
    "\u2013": "-",
    "\u00a0": " ",
}

SKIP_STRINGS=[
    "_",
    "normalized",
    "\\n",
    "Z - nds",
    "G -- ",
    "d - l",
    "This quote",
    "----",
    "si- lent",
]

SKIP_IF_NOT_EQUALLY_IN_BOTH = [
    "---",
    "[",
    "]",
    "...",
    "#",
    ".--"
]

REMOVE_IF_STARTS_WITH = [
    ", ",
    "-- ",
    ": ",
    ". ",
]

all_chunks_data = {}
all_sentence_pairs = {}
for file in os.listd...[truncated middle 8223 characters]...E_SWIFTIFIED_END
        full_block = prior_block1 + " " + sentence_block + " " + post_block + " " + SWIFTIFICATION_START + " " + swift_sentence + " " + SWIFTIFICATION_END
        final_dataset.append(full_block)
        full_block = prior_block1 + " " + sentence_block + " " + SWIFTIFICATION_START + " " + swift_sentence + " " + SWIFTIFICATION_END
        final_dataset.append(full_block)
        if prior_block1 != prior_block2:
            full_block = prior_block2 + " " + sentence_block + " " + post_block + " " + SWIFTIFICATION_START + " " + swift_sentence + " " + SWIFTIFICATION_END
            final_dataset.append(full_block)
            full_block = prior_block2 + " " + sentence_block + " " + SWIFTIFICATION_START + " " + swift_sentence + " " + SWIFTIFICATION_END
            final_dataset.append(full_block)
            

random.shuffle(final_dataset)
#save the dataset
with open(os.path.join(ebook_analyses_dir, "final_dataset.json"), "w") as f:
    json.dump(final_dataset, f, indent=4)

-----new_file-----
 /permathings/scripts/ebook_processing/convert_to_text.sh:
THIS_DIR_PATH=$(dirname $(realpath $0))
SCRIPTS_DIR_PATH=$(dirname $THIS_DIR_PATH)
PERMATHINGS_DIR_PATH=$(dirname $SCRIPTS_DIR_PATH)
ML_PROJECT_DIR_PATH=$(dirname $PERMATHINGS_DIR_PATH)
EPHEMERA_DIR_PATH=$ML_PROJECT_DIR_PATH/ephemera
INPUTS_DIR_PATH=$EPHEMERA_DIR_PATH/inputs
INPUT_EBOOKS_DIR_PATH=$INPUTS_DIR_PATH/ebooks
if [ ! -d $INPUT_EBOOKS_DIR_PATH ]; then
    mkdir -p $INPUT_EBOOKS_DIR_PATH
fi

for INPUT_EBOOK_PATH in $INPUT_EBOOKS_DIR_PATH/*.azw; do
    OUTPUT_EBOOK_PATH=$(echo $INPUT_EBOOK_PATH | sed 's/\.azw$/\.txt/')
    if [ -f $OUTPUT_EBOOK_PATH ]; then
        continue
    fi
    ebook-convert $INPUT_EBOOK_PATH $OUTPUT_EBOOK_PATH --enable-heuristics --replace-scene-breaks SCENE__BREAK__SCENE__BREAK
done


for INPUT_EBOOK_PATH in $INPUT_EBOOKS_DIR_PATH/*.epub; do
    OUTPUT_EBOOK_PATH=$(echo $INPUT_EBOOK_PATH | sed 's/\.epub$/\.txt/')
    if [ -f $OUTPUT_EBOOK_PATH ]; then
        continue
    fi
    ebook-convert $INPUT_EBOOK_PATH $OUTPUT_EBOOK_PATH --enable-heuristics --replace-scene-breaks SCENE__BREAK__SCENE__BREAK
done

-----new_file-----
 /permathings/scripts/ebook_processing/run_docker.sh:
THIS_DOCKER_CONTAINER_NAME="transformers-ebook-processing"

THIS_DIR_PATH=$(dirname $(realpath $0))
SCRIPTS_DIR_PATH=$(dirname $THIS_DIR_PATH)
PERMATHINGS_DIR_PATH=$(dirname $SCRIPTS_DIR_PATH)
LIBS_DIR_PATH=$PERMATHINGS_DIR_PATH/libs
ML_PROJECT_DIR_PATH=$(dirname $PERMATHINGS_DIR_PATH)
EPHEMERA_DIR_PATH=$ML_PROJECT_DIR_PATH/ephemera
INPUTS_DIR_PATH=$EPHEMERA_DIR_PATH/inputs
OUTPUTS_DIR_PATH=$EPHEMERA_DIR_PATH/outputs
EBOOK_ANALYSES_DIR_PATH=$OUTPUTS_DIR_PATH/ebook_analyses
INPUT_EBOOKS_DIR_PATH=$INPUTS_DIR_PATH/ebooks
SHARED_CACHES_DIR_PATH=$EPHEMERA_DIR_PATH/shared_caches
SECRETS_FILE_PATH=$EPHEMERA_DIR_PATH/secrets.json

HWID=$(hwid | cut -d ' ' -f 2)

if [ ! -d $OUTPUTS_DIR_PATH ]; then
    mkdir -p $OUTPUTS_DIR_PATH
fi
if [ ! -f $EBOOK_ANALYSES_DIR_PATH ]; then
    mkdir -p $EBOOK_ANALYSES_DIR_PATH
fi

cd $LIBS_DIR_PATH
python3 -c "from secretary import get_secret; n=get_secret('HF_TOKEN')"
cd $THIS_DIR_PATH

HF_TOKEN_ENCRYPTED=$(jq -r '.HF_TOKEN' $SECRETS_FILE_PATH)

HF_TOKEN=$(echo "$HF_TOKEN_ENCRYPTED" | openssl enc -d -aes-256-cbc -a -salt -pass pass:$HWID -pbkdf2)

docker build -t $THIS_DOCKER_CONTAINER_NAME .

if [ ! -d $SHARED_CACHES_DIR_PATH ]; then
    mkdir -p $SHARED_CACHES_DIR_PATH
fi

if [ ! -d $SHARED_CACHES_DIR_PATH/$THIS_DOCKER_CONTAINER_NAME ]; then
    mkdir -p $SHARED_CACHES_DIR_PATH/$THIS_DOCKER_CONTAINER_NAME
fi

WORKDIR_PATH=$THIS_DIR_PATH/workspace

if [ ! -d $WORKDIR_PATH ]; then
    mkdir -p $WORKDIR_PATH
fi

#echo "HF_TOKEN $HF_TOKEN"
docker run -it --gpus '"device=0"' --rm \
    --name $THIS_DOCKER_CONTAINER_NAME \
    -v $SHARED_CACHES_DIR_PATH/$THIS_DOCKER_CONTAINER_NAME:/ephemeral_cache \
    -v $WORKDIR_PATH:/workspace \
    -e HF_TOKEN=$HF_TOKEN \
    -v $INPUT_EBOOKS_DIR_PATH:/ebooks \
    -v $EBOOK_ANALYSES_DIR_PATH:/ebook_analyses \
    -w /workspace \
    $THIS_DOCKER_CONTAINER_NAME


-----new_file-----
 permathings/scripts/ebook_processing/workspace: 6 file(s) skipped in this folder

-----new_file-----
 /permathings/scripts/ebook_processing/workspace/test_outlines_function_split.py:
import os

#cuda visible devices= 0,1
os.environ["CUDA_VISIBLE_DEVICES"] = "0,1"

from pydantic import BaseModel
import json
import torch
from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer, logging
from accelerate import init_empty_weights, infer_auto_device_map, load_checkpoint_and_dispatch
torch.cuda.empty_cache()
import outlines


MODEL_DIR_PATH="/ephemeral_cache/mistral-inst-v03"


function_json_string = '''{
    "type": "function",
    "function": {
        "name": "define_section_splits",
        "description": "Store data related to the sectional divisions of a text",
        "parameters": {
            "type": "object",
            "properties": {
                "section_divisions": {
                    "type": "array",
                    "items": {
                        "type": "object",
                        "properties": {
                            "division_type": {
                                "type": "string",
                            ...[truncated middle 32307 characters]...om_config(config)

print("infer auto device map")

max_memory = {0:"4.5GiB", 1:"4.5GiB","cpu":"0GiB"}


print("max_memory", max_memory)

device_map = infer_auto_device_map(
        model,
        max_memory=max_memory,
        no_split_module_classes=["MistralDecoderLayer"],
        dtype=torch.int8
    )

#print(device_map)

print("load checkpoint and dispatch")
model = outlines.models.transformers(
        MODEL_DIR_PATH,
        #device="cuda",   #why... https://github.com/outlines-dev/outlines/blob/a987159860a6dd3a83d2f2376f36ab28ef45decd/outlines/models/transformers.py#L229
        device=None,
        model_kwargs={
            "torch_dtype": torch.bfloat16,
            "device_map": device_map,
            "load_in_8bit": True,
            "config":config,
        },
    )
generator = outlines.generate.json(model, function_params_string)
output = generator(prompt)

print(json.dumps(output, indent=2))

with open('example_output.json', 'w') as f:
    json.dump(output, f, indent=2)

-----new_file-----
 /permathings/scripts/ebook_processing/workspace/verify_swiftian.py:
import os, sys, re, json, random, time
from unidecode import unidecode

#cuda visible devices= 0,1
#os.environ["CUDA_VISIBLE_DEVICES"] = "0,1"
os.environ["CUDA_VISIBLE_DEVICES"] = "0"

from pydantic import BaseModel
import torch
from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer, logging
from accelerate import init_empty_weights, infer_auto_device_map, load_checkpoint_and_dispatch
torch.cuda.empty_cache()
import outlines


MODEL_DIR_PATH="/ephemeral_cache/mistral-inst-v03"
LOAD_MODEL = True


ebook_dir = "/ebooks"
ebook_analysis_dir = "/ebook_analyses"
swift_quotes_dir = ebook_analysis_dir+"/swift_quotes"
if not os.path.exists(swift_quotes_dir):
    os.makedirs(swift_quotes_dir)
jswift_book = "jswift.txt"
jswift_path = os.path.join(ebook_dir, jswift_book)
jswift_string = open(jswift_path).read()

scene_split_lines = jswift_string.split("SCENE__BREAK__SCENE__BREAK")

reduced_lines = []
for line in scene_split_lines:
    if len(line) < 6000:
        continue
    lin...[truncated middle 11681 characters]...ator = outlines.generate.json(model, function_params_string)
        tools_string="[AVAILABLE_TOOLS] ["+this_function_string+"][/AVAILABLE_TOOLS]"
        instruction_string="[INST] "+ instruction + " [/INST]"
        prompt = tools_string + instruction_string + "[TOOL_CALLS]"
        output = generator(prompt)
        try:
            output_json = json.loads(output)
        except:
            output_json = output
        data_to_save = {
            "prompt_string": prompt,
            "given_info_as_json_dict": given_info_as_json_dict,
            "function": this_function_json,
            "output": output,
            "output_json": output_json,
        }
        with open(save_path, "w") as f:
            json.dump(data_to_save, f, indent=4)
        print("Saved chunk", k, "to", save_path)
    except Exception as e:
        #if keyboard interrupt, save progress
        if "KeyboardInterrupt" in str(e):
            break
        print(e)
        print("Error processing chunk", k)

-----new_file-----
 permathings/scripts/finetuning: Empty folder or all files filtered

-----new_file-----
 permathings/scripts/finetuning/simple_ft: 4 file(s) skipped in this folder

-----new_file-----
 /permathings/scripts/finetuning/simple_ft/build.sh:
docker build -t finetune .

-----new_file-----
 /permathings/scripts/finetuning/simple_ft/merge_run.sh:
THIS_DIR_PATH=$(dirname $(realpath $0))
SCRIPTS_DIR_PATH=$(dirname $(dirname $THIS_DIR_PATH))
PERMATHINGS_DIR_PATH=$(dirname $SCRIPTS_DIR_PATH)
LIBRARIES_DIR_PATH=$PERMATHINGS_DIR_PATH/libs
ML_PROJECT_DIR_PATH=$(dirname $PERMATHINGS_DIR_PATH)
EPHEMERA_DIR_PATH=$ML_PROJECT_DIR_PATH/ephemera
SHARED_CACHES_DIR_PATH=$EPHEMERA_DIR_PATH/shared_caches

#execute print(get_secret("HF_TOKEN")) from secretary in python
cd $LIBRARIES_DIR_PATH
HF_TOKEN=$(python3 -c "from secretary import get_secret; print(get_secret('HF_TOKEN'))")
cd $THIS_DIR_PATH

HF_CACHE_FOLDER_PATH=$SHARED_CACHES_DIR_PATH/huggingface

#docker run --rm --gpus '"device=0,1"' \
docker run --rm --gpus '"device=1"' \
    -e HF_TOKEN=$HF_TOKEN \
    -v ./workdir:/workdir \
    -w /workdir \
    -v $HF_CACHE_FOLDER_PATH:/root/.cache/huggingface \
    --name merge \
    -it finetune 

-----new_file-----
 permathings/scripts/finetuning/simple_ft/workdir: 4 file(s) skipped in this folder

-----new_file-----
 /permathings/scripts/finetuning/simple_ft/workdir/finetune_depr.py:
import json, os
import torch
from datasets import Dataset
from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling
from peft import LoraConfig, get_peft_model,prepare_model_for_kbit_training
from bitsandbytes.optim import Adam8bit
import datetime
import transformers
import bitsandbytes
from transformers.integrations import CodeCarbonCallback

RESUME_IF_CKPT = False

# Load dataset from JSON file
#YYMMDD_support_test
#TOKENIZERS_PARALLELISM=false
os.environ["TOKENIZERS_PARALLELISM"] = "false"
#disable warnings
torch.autograd.set_detect_anomaly(True)
transformers.logging.set_verbosity_error()

import warnings
warnings.filterwarnings('ignore')


out_directory_name = datetime.datetime.now().strftime("%y%m%d") + "_support_bot/"
training_data_path = "/dataset.json"

with open(training_data_path, "r") as f:
    train_data = json.load(f)

full_dataset = Dataset.from_dict({"text": train_data})

q4_config = transformers.BitsAndByte...[truncated middle 1996 characters]...    #gradient_checkpointing=True,
    num_train_epochs=1,
    learning_rate=learning_rate,
    fp16=True,
    #bf16=True,
    logging_steps=1,
    save_strategy="steps",
    save_steps=40,
    save_total_limit=10,
    overwrite_output_dir=True,
    evaluation_strategy= "no",
)

optimizer = Adam8bit(model.parameters(), lr=learning_rate)

#clear cache before training
torch.cuda.empty_cache()

# Set up Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=full_dataset,
    data_collator=data_collator,
)

trainer.remove_callback(CodeCarbonCallback)

resume_from_checkpoint = False
if os.path.exists(ckpt_dir) and RESUME_IF_CKPT:
    resume_from_checkpoint = True
# Fine-tune model
trainer.train(resume_from_checkpoint=resume_from_checkpoint)

output_dir = os.path.join(out_dir_path, "output")

# Save fine-tuned model

try:
    model.save_pretrained(output_dir)
    tokenizer.save_pretrained(output_dir)
except:
    print("Error saving model and tokenizer")
    pass

-----new_file-----
 /permathings/scripts/finetuning/simple_ft/workdir/finetune_swift.py:
import json, os
import torch
from datasets import Dataset
from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling
from peft import LoraConfig, get_peft_model,prepare_model_for_kbit_training
from bitsandbytes.optim import Adam8bit
import datetime
import transformers
import bitsandbytes
from transformers.integrations import CodeCarbonCallback

import torch
from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer, logging
from accelerate import init_empty_weights, infer_auto_device_map, load_checkpoint_and_dispatch

RESUME_IF_CKPT = False

# Load dataset from JSON file
#YYMMDD_support_test
#TOKENIZERS_PARALLELISM=false
os.environ["TOKENIZERS_PARALLELISM"] = "false"
#disable warnings
torch.autograd.set_detect_anomaly(True)
transformers.logging.set_verbosity_error()

import warnings
warnings.filterwarnings('ignore')


out_directory_name = "/workdir/"+datetime.datetime.now().strftime("%y%m%d") + "_swift2/"
trai...[truncated middle 3055 characters]...ting=True,
    num_train_epochs=1,
    learning_rate=learning_rate,
    #fp16=True,
    #bf16=True,
    logging_steps=1,
    save_strategy="steps",
    save_steps=40,
    save_total_limit=10,
    overwrite_output_dir=True,
    evaluation_strategy= "no",
    report_to="none",
)

optimizer = Adam8bit(model.parameters(), lr=learning_rate)

#clear cache before training
torch.cuda.empty_cache()

# Set up Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=full_dataset,
    data_collator=data_collator,
)

trainer.remove_callback(CodeCarbonCallback)

resume_from_checkpoint = False
if os.path.exists(ckpt_dir) and RESUME_IF_CKPT:
    resume_from_checkpoint = True
# Fine-tune model
trainer.train(resume_from_checkpoint=resume_from_checkpoint)

output_dir = os.path.join(out_dir_path, "output")

# Save fine-tuned model

try:
    model.save_pretrained(output_dir)
    tokenizer.save_pretrained(output_dir)
except:
    print("Error saving model and tokenizer")
    pass

-----new_file-----
 /permathings/scripts/langchain/Dockerfile:
FROM transformers-quantization-latest-gpu

RUN apt

-----new_file-----
 /permathings/scripts/langchain/run.sh:
THIS_DOCKER_CONTAINER_NAME="transformers-quantization-latest-gpu"

THIS_DIR_PATH=$(dirname $(realpath $0))
SCRIPTS_DIR_PATH=$(dirname $THIS_DIR_PATH)
PERMATHINGS_DIR_PATH=$(dirname $SCRIPTS_DIR_PATH)
ML_PROJECT_DIR_PATH=$(dirname $PERMATHINGS_DIR_PATH)
EPHEMERA_DIR_PATH=$ML_PROJECT_DIR_PATH/ephemera
SHARED_CACHES_DIR_PATH=$EPHEMERA_DIR_PATH/shared_caches

if [ ! -f ./Dockerfile ]; then
    wget https://raw.githubusercontent.com/huggingface/transformers/main/docker/transformers-quantization-latest-gpu/Dockerfile 
fi

docker build -t $THIS_DOCKER_CONTAINER_NAME .

if [ ! -d $SHARED_CACHES_DIR_PATH ]; then
    mkdir -p $SHARED_CACHES_DIR_PATH
fi

if [ ! -d $SHARED_CACHES_DIR_PATH/$THIS_DOCKER_CONTAINER_NAME ]; then
    mkdir -p $SHARED_CACHES_DIR_PATH/$THIS_DOCKER_CONTAINER_NAME
fi

WORKDIR_PATH=$THIS_DIR_PATH/workspace

if [ ! -d $WORKDIR_PATH ]; then
    mkdir -p $WORKDIR_PATH
fi

docker run -it --gpus all --rm \
    --name $THIS_DOCKER_CONTAINER_NAME \
    -v $SHARED_CACHES_DIR_PATH/$THIS_DOCKER_CONTAINER_NAME:/root/.cache \
    -v $WORKDIR_PATH:/workspace \
    $THIS_DOCKER_CONTAINER_NAME


-----new_file-----
 permathings/scripts/ollama: 1 file(s) skipped in this folder

-----new_file-----
 /permathings/scripts/ollama/describe_screenshot.sh:
THIS_MODEL="llava:34b-v1.6"

#THIS_MODEL="llava-phi3"

json_string="{'model': '$THIS_MODEL', 'prompt': 'What is in this picture?', 'stream': False, 'images': [base64.b64encode(open('"screenshot.png"', 'rb').read()).decode('utf-8')]}).json()"
echo $json_string
command_string="import requests, base64, json; print(json.dumps(requests.post('http://localhost:11434/api/generate', json="$json_string", indent=2))"
response=$(python3 -c "$command_string")
echo $response | jq . > screenshot.png.json


-----new_file-----
 /permathings/scripts/ollama/example_output/0811wallpaper-7_1280.jpg.json:
{
  "model": "llava:34b-v1.6",
  "created_at": "2024-06-10T17:09:23.18214022Z",
  "response": "The image shows a bird standing on the surface of what appears to be water. The bird has distinctive markings, with a black head and chest, white on its wings and back, and a reddish-brown belly. Its eye is brightly colored in contrast to the rest of its plumage. The bird seems calm as it stands still in the middle of the water body. The photo also contains metadata indicating it was taken by a photographer named Joel Vargas and is copyrighted material from 2007.",
  "done": true,
  "done_reason": "stop",
  "context": [
    1581,
    59705,
    622,
    59593,
    5858,
    46826,
    10707,
    144,
    59666,
    59705,
    622,
    59593,
    701,
    46826,
    144,
    59666,
    59705,
    622,
    59593,
    5858,
    46826,
    3903,
    144,
    5697,
    620,
    594,
    719,
    4652,
    100,
    59666,
    59705,
    622,
    59593,
    701,
    46826,
    144,
    59666,
    59...[truncated middle 229 characters]...  707,
    13359,
    815,
    24581,
    55116,
    97,
    651,
    562,
    2876,
    1806,
    597,
    12640,
    97,
    3589,
    632,
    1034,
    19546,
    597,
    1122,
    97,
    597,
    562,
    15153,
    1078,
    59594,
    46912,
    24246,
    98,
    7562,
    5597,
    620,
    5975,
    627,
    18431,
    594,
    6889,
    592,
    567,
    1797,
    593,
    1034,
    39598,
    807,
    98,
    707,
    13359,
    2833,
    13905,
    659,
    648,
    8954,
    1451,
    594,
    567,
    4866,
    593,
    567,
    2127,
    2534,
    98,
    707,
    5631,
    962,
    5007,
    22468,
    15250,
    648,
    717,
    2955,
    737,
    562,
    18610,
    5582,
    34002,
    1008,
    1406,
    599,
    597,
    620,
    38198,
    2920,
    742,
    59568,
    79,
    77,
    77,
    84,
    98
  ],
  "total_duration": 54758790095,
  "load_duration": 2157893,
  "prompt_eval_duration": 17319623000,
  "eval_count": 110,
  "eval_duration": 37375941000
}


-----new_file-----
 /permathings/scripts/ollama/example_output/0825wallpaper-13_1280.jpg.json:
{
  "model": "llava:34b-v1.6",
  "created_at": "2024-06-10T17:50:25.533009278Z",
  "response": "This picture shows a scenic winter landscape with a snow-covered ground and trees. In the center of the image, there is a large circular pool that appears to be steaming or emitting vapor, indicating that it's likely a hot spring despite the cold surrounding environment. The contrast between the frozen surroundings and the heat from the hot springs creates an interesting juxtaposition within the scene. There are also clouds visible in the sky above, adding to the dramatic effect of the image. The photo is credited to \"Gary Leland\" with a timestamp indicating it was taken on August 26, 2010.",
  "done": true,
  "done_reason": "stop",
  "context": [
    1581,
    59705,
    622,
    59593,
    5858,
    46826,
    10707,
    144,
    59666,
    59705,
    622,
    59593,
    701,
    46826,
    144,
    59666,
    59705,
    622,
    59593,
    5858,
    46826,
    3903,
    144,
    5697,
 ...[truncated middle 584 characters]...    15250,
    639,
    648,
    59610,
    59575,
    3008,
    562,
    2987,
    7268,
    6285,
    567,
    5711,
    9929,
    3247,
    98,
    707,
    6889,
    1397,
    567,
    16402,
    26199,
    597,
    567,
    5332,
    742,
    567,
    2987,
    31903,
    10626,
    663,
    3571,
    9168,
    852,
    744,
    30112,
    2050,
    567,
    5753,
    98,
    1889,
    678,
    962,
    16529,
    9686,
    594,
    567,
    8697,
    2198,
    97,
    5952,
    592,
    567,
    14868,
    1646,
    593,
    567,
    2728,
    98,
    707,
    5631,
    620,
    31401,
    592,
    1529,
    59628,
    898,
    750,
    16409,
    59592,
    651,
    562,
    35541,
    15250,
    648,
    717,
    2955,
    632,
    5143,
    59568,
    79,
    83,
    97,
    59568,
    79,
    77,
    78,
    77,
    98
  ],
  "total_duration": 67881211735,
  "load_duration": 3330098,
  "prompt_eval_duration": 17181581000,
  "eval_count": 133,
  "eval_duration": 50627769000
}


-----new_file-----
 permathings/scripts/ollama/example_output: 73 file(s) skipped in this folder

-----new_file-----
 /permathings/scripts/ollama/run_llava.sh:
THIS_MODEL="llava:34b-v1.6"

#THIS_MODEL="llava-phi3"

THIS_DIR_PATH=$(dirname $(realpath $0))
SCRIPTS_DIR_PATH=$(dirname $THIS_DIR_PATH)
PERMATHINGS_DIR_PATH=$(dirname $SCRIPTS_DIR_PATH)
ML_PROJECT_DIR_PATH=$(dirname $PERMATHINGS_DIR_PATH)
EPHEMERA_DIR_PATH=$ML_PROJECT_DIR_PATH/ephemera
SHARED_CACHES_DIR_PATH=$EPHEMERA_DIR_PATH/shared_caches
OLLAMA_CACHE_FOLDER_PATH=$SHARED_CACHES_DIR_PATH/ollama

docker kill ollama
docker rm ollama
docker kill /ollama
docker rm /ollama

docker run -d --rm --gpus all \
    -v $OLLAMA_CACHE_FOLDER_PATH:/root/.ollama -p 11434:11434 --name ollama ollama/ollama

docker exec ollama ollama run $THIS_MODEL

-----new_file-----
 /permathings/scripts/ollama/run_web_bundle.sh:
THIS_DIR_PATH=$(dirname $(realpath $0))
SCRIPTS_DIR_PATH=$(dirname $THIS_DIR_PATH)
PERMATHINGS_DIR_PATH=$(dirname $SCRIPTS_DIR_PATH)
ML_PROJECT_DIR_PATH=$(dirname $PERMATHINGS_DIR_PATH)
EPHEMERA_DIR_PATH=$ML_PROJECT_DIR_PATH/ephemera
SHARED_CACHES_DIR_PATH=$EPHEMERA_DIR_PATH/shared_caches
OLLAMA_CACHE_FOLDER_PATH=$SHARED_CACHES_DIR_PATH/ollama
OLLAMA_WEB_FOLDER_PATH=$SHARED_CACHES_DIR_PATH/ollama_web

docker kill open-webui
docker rm open-webui
sleep 3

docker run -d -p 3000:8080 --gpus '"device=0,1"' -v $OLLAMA_CACHE_FOLDER_PATH:/root/.ollama \
    -v $OLLAMA_WEB_FOLDER_PATH:/app/backend/data --name open-webui --restart always \
    -e OLLAMA_HOST=0.0.0.0 \
    ghcr.io/open-webui/open-webui:ollama


-----new_file-----
 /permathings/scripts/ollama/screenshot.png:
(Binary file)

-----new_file-----
 /permathings/scripts/ollama/screenshot.png.json:
{
  "model": "llava:34b-v1.6",
  "created_at": "2024-06-11T14:56:49.970430224Z",
  "response": "The image shows a webpage for a digital storage unit converter. It appears to be a tool that allows users to convert between different units of data storage, such as megabytes (MB), gigabytes (GB), terabytes (TB), and other related terms. The page includes instructions or tips on how to use the calculator, which are written in smaller text below the main conversion area. There's a button labeled \"Calculate\" that presumably performs the data storage conversion when clicked. The interface is simple and seems to be designed for quick calculations of digital storage sizes.",
  "done": true,
  "done_reason": "stop",
  "context": [
    1581,
    59705,
    622,
    59593,
    5858,
    46826,
    10707,
    144,
    59666,
    59705,
    622,
    59593,
    701,
    46826,
    144,
    59666,
    59705,
    622,
    59593,
    5858,
    46826,
    3903,
    144,
    5697,
    620,
    594,
    7...[truncated middle 507 characters]... 4267,
    6382,
    662,
    9812,
    59604,
    97,
    19267,
    4267,
    6382,
    662,
    6021,
    59604,
    97,
    5019,
    4267,
    6382,
    662,
    25151,
    59604,
    97,
    597,
    924,
    3691,
    2818,
    98,
    707,
    2943,
    3849,
    9745,
    705,
    7610,
    632,
    1040,
    592,
    1149,
    567,
    41675,
    97,
    878,
    678,
    3738,
    594,
    4908,
    2641,
    2723,
    567,
    1740,
    13078,
    2259,
    98,
    1889,
    59610,
    59575,
    562,
    6197,
    15815,
    1529,
    13525,
    16643,
    59592,
    639,
    25946,
    16331,
    567,
    1394,
    6526,
    13078,
    950,
    30663,
    98,
    707,
    6780,
    620,
    2812,
    597,
    2833,
    592,
    629,
    4060,
    631,
    2741,
    9830,
    593,
    5439,
    6526,
    9167,
    98
  ],
  "total_duration": 45721774587,
  "load_duration": 3763409,
  "prompt_eval_duration": 9237577000,
  "eval_count": 126,
  "eval_duration": 36451981000
}


-----new_file-----
 permathings/scripts/segmentation: Empty folder or all files filtered

-----new_file-----
 permathings/scripts/segmentation/OpenAdapt: 1 file(s) skipped in this folder

-----new_file-----
 /permathings/scripts/segmentation/OpenAdapt/grab_weights.sh:
THIS_DIR_PATH=$(dirname $(realpath $0))
SCRIPTS_DIR_PATH=$(dirname $(dirname $THIS_DIR_PATH))
PERMATHINGS_DIR_PATH=$(dirname $SCRIPTS_DIR_PATH)
ML_PROJECT_DIR_PATH=$(dirname $PERMATHINGS_DIR_PATH)
EPHEMERA_DIR_PATH=$ML_PROJECT_DIR_PATH/ephemera
SHARED_CACHES_DIR_PATH=$EPHEMERA_DIR_PATH/shared_caches

if [ ! -d $SHARED_CACHES_DIR_PATH ]; then
    mkdir -p $SHARED_CACHES_DIR_PATH
fi
cd $SHARED_CACHES_DIR_PATH

git lfs install
git clone https://huggingface.co/xk-huang/segment-caption-anything-ollm3bv2-vg

-----new_file-----
 /permathings/scripts/segmentation/OpenAdapt/run_docker.sh:
THIS_DOCKER_CONTAINER_NAME="transformers-openadapt"

THIS_DIR_PATH=$(dirname $(realpath $0))
SCRIPTS_DIR_PATH=$(dirname $(dirname $THIS_DIR_PATH))
TEMPLATE_DIR_PATH=$SCRIPTS_DIR_PATH/tformers_template
TEMPLATE_IMAGE_NAME="transformers-quantization-latest-gpu"
PERMATHINGS_DIR_PATH=$(dirname $SCRIPTS_DIR_PATH)
ML_PROJECT_DIR_PATH=$(dirname $PERMATHINGS_DIR_PATH)
EPHEMERA_DIR_PATH=$ML_PROJECT_DIR_PATH/ephemera
SHARED_CACHES_DIR_PATH=$EPHEMERA_DIR_PATH/shared_caches

docker build -t $THIS_DOCKER_CONTAINER_NAME .

if [ ! -d $SHARED_CACHES_DIR_PATH ]; then
    mkdir -p $SHARED_CACHES_DIR_PATH
fi

THIS_CACHE=$SHARED_CACHES_DIR_PATH/$THIS_DOCKER_CONTAINER_NAME

if [ ! -d $THIS_CACHE ]; then
    mkdir -p $THIS_CACHE
fi

WORKDIR_PATH=$THIS_DIR_PATH/workspace

if [ ! -d $WORKDIR_PATH ]; then
    mkdir -p $WORKDIR_PATH
fi

docker kill $THIS_DOCKER_CONTAINER_NAME
docker rm $THIS_DOCKER_CONTAINER_NAME
sleep 2
docker kill $THIS_DOCKER_CONTAINER_NAME
docker rm $THIS_DOCKER_CONTAINER_NAME
sleep 2

docker run --gpus all -it --rm \
    --name $THIS_DOCKER_CONTAINER_NAME \
    -v $THIS_CACHE:/root/.cache \
    -v $WORKDIR_PATH:/workspace \
    -w /workspace \
    -p 7860:7860 \
    $THIS_DOCKER_CONTAINER_NAME


-----new_file-----
 /permathings/scripts/segmentation/OpenAdapt/workspace/run.sh:


-----new_file-----
 /permathings/scripts/segmentation/SAM/Dockerfile:
FROM transformers-quantization-latest-gpu

# Install the necessary packages
RUN apt update
RUN apt install -y git
RUN python3 -m pip install --no-cache-dir --upgrade pip
RUN python3 -m pip install --no-cache-dir opencv-python
RUN python3 -m pip install --no-cache-dir numpy
RUN python3 -m pip install --no-cache-dir pillow
RUN python3 -m pip install --no-cache-dir matplotlib

-----new_file-----
 /permathings/scripts/segmentation/SAM/run.sh:
THIS_DOCKER_CONTAINER_NAME="transformers-for-sam"

THIS_DIR_PATH=$(dirname $(realpath $0))
SCRIPTS_DIR_PATH=$(dirname $(dirname $THIS_DIR_PATH))
TEMPLATE_DIR_PATH=$SCRIPTS_DIR_PATH/tformers_template
TEMPLATE_IMAGE_NAME="transformers-quantization-latest-gpu"
PERMATHINGS_DIR_PATH=$(dirname $SCRIPTS_DIR_PATH)
ML_PROJECT_DIR_PATH=$(dirname $PERMATHINGS_DIR_PATH)
EPHEMERA_DIR_PATH=$ML_PROJECT_DIR_PATH/ephemera
SHARED_CACHES_DIR_PATH=$EPHEMERA_DIR_PATH/shared_caches

docker build -t $TEMPLATE_IMAGE_NAME $TEMPLATE_DIR_PATH
docker build -t $THIS_DOCKER_CONTAINER_NAME .

if [ ! -d $SHARED_CACHES_DIR_PATH ]; then
    mkdir -p $SHARED_CACHES_DIR_PATH
fi

THIS_CACHE=$SHARED_CACHES_DIR_PATH/$THIS_DOCKER_CONTAINER_NAME

if [ ! -d $THIS_CACHE ]; then
    mkdir -p $THIS_CACHE
fi

WORKDIR_PATH=$THIS_DIR_PATH/workspace

if [ ! -d $WORKDIR_PATH ]; then
    mkdir -p $WORKDIR_PATH
fi

docker kill $THIS_DOCKER_CONTAINER_NAME
docker rm $THIS_DOCKER_CONTAINER_NAME
sleep 2
docker kill $THIS_DOCKER_CONTAINER_NAME
docker rm $THIS_DOCKER_CONTAINER_NAME
sleep 2

docker run --gpus all -it --rm \
    --name $THIS_DOCKER_CONTAINER_NAME \
    -v $THIS_CACHE:/root/.cache \
    -v $WORKDIR_PATH:/workspace \
    -w /workspace \
    -p 8000:8000 \
    $THIS_DOCKER_CONTAINER_NAME


-----new_file-----
 permathings/scripts/segmentation/SAM/workspace: 3 file(s) skipped in this folder

-----new_file-----
 /permathings/scripts/segmentation/SAM/workspace/out.png:
(Binary file)

-----new_file-----
 /permathings/scripts/segmentation/SAM/workspace/test1.py:
IMAGE_URL = """https://ia802906.us.archive.org/13/items/artworks1/The%20Voyage%20of%20Life%20-%203%20-%20Manhood%20-%20Thomas%20Cole%2C%201842.jpg"""
OUT_FILENAME = "mhood.jpg"
LABELS = ["a man.", "a tree.", "the sea."]

import random
from dataclasses import dataclass
from typing import Any, List, Dict, Optional, Union, Tuple

import cv2
import torch
import requests
import numpy as np
from PIL import Image
import plotly.express as px
import matplotlib.pyplot as plt
import plotly.graph_objects as go
from transformers import AutoModelForMaskGeneration, AutoProcessor, pipeline

#grab the image first, if it's not already in the directory
import os
import urllib.request
if not os.path.exists(OUT_FILENAME):
    urllib.request.urlretrieve(IMAGE_URL, OUT_FILENAME)

@dataclass
class BoundingBox:
    xmin: int
    ymin: int
    xmax: int
    ymax: int

    @property
    def xyxy(self) -> List[float]:
        return [self.xmin, self.ymin, self.xmax, self.ymax]

@dataclass
class DetectionResult:
 ...[truncated middle 12091 characters]...etector_id)
    detections = segment(image, detections, polygon_refinement, segmenter_id)

    return np.array(image), detections

"""### Inference

Let's showcase Grounded SAM on our favorite image: the cats image from the COCO dataset.
"""

#image_url = "http://images.cocodataset.org/val2017/000000039769.jpg"
#labels = ["a cat.", "a remote control."]
image_url = OUT_FILENAME
labels = LABELS
threshold = 0.3

noext_filename = ".".join(OUT_FILENAME.split(".")[:-1])
extension = OUT_FILENAME.split(".")[-1]
OUT_FILENAME = f"{noext_filename}_grounded.{extension}"

detector_id = "IDEA-Research/grounding-dino-tiny"
segmenter_id = "facebook/sam-vit-base"

image_array, detections = grounded_segmentation(
    image=image_url,
    labels=labels,
    threshold=threshold,
    polygon_refinement=True,
    detector_id=detector_id,
    segmenter_id=segmenter_id
)

"""Let's visualize the results:"""

plot_detections(image_array, detections, OUT_FILENAME)

plot_detections_plotly(image_array, detections)

-----new_file-----
 permathings/scripts/segmentation/segment-caption-anything: 1 file(s) skipped in this folder

-----new_file-----
 /permathings/scripts/segmentation/segment-caption-anything/grab_weights.sh:
THIS_DIR_PATH=$(dirname $(realpath $0))
SCRIPTS_DIR_PATH=$(dirname $(dirname $THIS_DIR_PATH))
PERMATHINGS_DIR_PATH=$(dirname $SCRIPTS_DIR_PATH)
ML_PROJECT_DIR_PATH=$(dirname $PERMATHINGS_DIR_PATH)
EPHEMERA_DIR_PATH=$ML_PROJECT_DIR_PATH/ephemera
SHARED_CACHES_DIR_PATH=$EPHEMERA_DIR_PATH/shared_caches

if [ ! -d $SHARED_CACHES_DIR_PATH ]; then
    mkdir -p $SHARED_CACHES_DIR_PATH
fi
cd $SHARED_CACHES_DIR_PATH

git lfs install
git clone https://huggingface.co/xk-huang/segment-caption-anything-ollm3bv2-vg

-----new_file-----
 /permathings/scripts/segmentation/segment-caption-anything/run_docker.sh:
THIS_DOCKER_CONTAINER_NAME="transformers-for-captioning-sam"

THIS_DIR_PATH=$(dirname $(realpath $0))
SCRIPTS_DIR_PATH=$(dirname $(dirname $THIS_DIR_PATH))
TEMPLATE_DIR_PATH=$SCRIPTS_DIR_PATH/tformers_template
TEMPLATE_IMAGE_NAME="transformers-quantization-latest-gpu"
PERMATHINGS_DIR_PATH=$(dirname $SCRIPTS_DIR_PATH)
ML_PROJECT_DIR_PATH=$(dirname $PERMATHINGS_DIR_PATH)
EPHEMERA_DIR_PATH=$ML_PROJECT_DIR_PATH/ephemera
SHARED_CACHES_DIR_PATH=$EPHEMERA_DIR_PATH/shared_caches

docker build -t $TEMPLATE_IMAGE_NAME $TEMPLATE_DIR_PATH
docker build -t $THIS_DOCKER_CONTAINER_NAME .

if [ ! -d $SHARED_CACHES_DIR_PATH ]; then
    mkdir -p $SHARED_CACHES_DIR_PATH
fi

THIS_CACHE=$SHARED_CACHES_DIR_PATH/$THIS_DOCKER_CONTAINER_NAME

if [ ! -d $THIS_CACHE ]; then
    mkdir -p $THIS_CACHE
fi

WORKDIR_PATH=$THIS_DIR_PATH/workspace

if [ ! -d $WORKDIR_PATH ]; then
    mkdir -p $WORKDIR_PATH
fi

docker kill $THIS_DOCKER_CONTAINER_NAME
docker rm $THIS_DOCKER_CONTAINER_NAME
sleep 2
docker kill $THIS_DOCKER_CONTAINER_NAME
docker rm $THIS_DOCKER_CONTAINER_NAME
sleep 2

CKPTS_DIR_PATH=$SHARED_CACHES_DIR_PATH/segment-caption-anything-ollm3bv2-vg

docker run --gpus all -it --rm \
    --name $THIS_DOCKER_CONTAINER_NAME \
    -v $THIS_CACHE:/root/.cache \
    -v $WORKDIR_PATH:/workspace \
    -v $CKPTS_DIR_PATH:/ckpts \
    -w /workspace \
    -p 7860:7860 \
    $THIS_DOCKER_CONTAINER_NAME


-----new_file-----
 /permathings/scripts/segmentation/segment-caption-anything/workspace/run.sh:
CKPT_PATH=/ckpts
cd /app/segment-caption-anything
python scripts/apps/sca_app.py \
+model=base_sca_multitask_v2 \
model.model_name_or_path=$CKPT_PATH \
model.lm_head_model_name_or_path=$(python scripts/tools/get_sub_model_name_from_ckpt.py $CKPT_PATH "lm")

-----new_file-----
 permathings/scripts/segmentation/segment-track-anything: 1 file(s) skipped in this folder

-----new_file-----
 /permathings/scripts/segmentation/segment-track-anything/grab_weights.sh:
THIS_DIR_PATH=$(dirname $(realpath $0))
SCRIPTS_DIR_PATH=$(dirname $(dirname $THIS_DIR_PATH))
PERMATHINGS_DIR_PATH=$(dirname $SCRIPTS_DIR_PATH)
ML_PROJECT_DIR_PATH=$(dirname $PERMATHINGS_DIR_PATH)
EPHEMERA_DIR_PATH=$ML_PROJECT_DIR_PATH/ephemera
SHARED_CACHES_DIR_PATH=$EPHEMERA_DIR_PATH/shared_caches

if [ ! -d $SHARED_CACHES_DIR_PATH ]; then
    mkdir -p $SHARED_CACHES_DIR_PATH
fi
cd $SHARED_CACHES_DIR_PATH

git clone https://github.com/z-x-yang/Segment-and-Track-Anything.git

cd Segment-and-Track-Anything

#bash script/install.sh

mkdir ./ckpt

bash script/download_ckpt.sh



-----new_file-----
 /permathings/scripts/segmentation/segment-track-anything/run_docker.sh:
THIS_DOCKER_CONTAINER_NAME="transformers-for-tracking-sam"

THIS_DIR_PATH=$(dirname $(realpath $0))
SCRIPTS_DIR_PATH=$(dirname $(dirname $THIS_DIR_PATH))
TEMPLATE_DIR_PATH=$SCRIPTS_DIR_PATH/tformers_template
TEMPLATE_IMAGE_NAME="transformers-quantization-latest-gpu"
PERMATHINGS_DIR_PATH=$(dirname $SCRIPTS_DIR_PATH)
ML_PROJECT_DIR_PATH=$(dirname $PERMATHINGS_DIR_PATH)
EPHEMERA_DIR_PATH=$ML_PROJECT_DIR_PATH/ephemera
SHARED_CACHES_DIR_PATH=$EPHEMERA_DIR_PATH/shared_caches

docker build -t $TEMPLATE_IMAGE_NAME $TEMPLATE_DIR_PATH
docker build -t $THIS_DOCKER_CONTAINER_NAME .

if [ ! -d $SHARED_CACHES_DIR_PATH ]; then
    mkdir -p $SHARED_CACHES_DIR_PATH
fi

THIS_CACHE=$SHARED_CACHES_DIR_PATH/$THIS_DOCKER_CONTAINER_NAME

if [ ! -d $THIS_CACHE ]; then
    mkdir -p $THIS_CACHE
fi

WORKDIR_PATH=$THIS_DIR_PATH/workspace

if [ ! -d $WORKDIR_PATH ]; then
    mkdir -p $WORKDIR_PATH
fi

docker kill $THIS_DOCKER_CONTAINER_NAME
docker rm $THIS_DOCKER_CONTAINER_NAME
sleep 2
docker kill $THIS_DOCKER_CONTAINER_NAME
docker rm $THIS_DOCKER_CONTAINER_NAME
sleep 2

REPO_PATH=$SHARED_CACHES_DIR_PATH/Segment-and-Track-Anything

docker run --gpus all -it --rm \
    --name $THIS_DOCKER_CONTAINER_NAME \
    -v $THIS_CACHE:/root/.cache \
    -v $WORKDIR_PATH:/workspace \
    -v $REPO_PATH:/app \
    -w /workspace \
    -p 7860:7860 \
    $THIS_DOCKER_CONTAINER_NAME


-----new_file-----
 /permathings/scripts/stable_diffusion_3/run.sh:
THIS_DIR_PATH=$(dirname $(realpath $0))
SCRIPTS_DIR_PATH=$(dirname $THIS_DIR_PATH)
PERMATHINGS_DIR_PATH=$(dirname $SCRIPTS_DIR_PATH)
LIBRARIES_DIR_PATH=$PERMATHINGS_DIR_PATH/libs
ML_PROJECT_DIR_PATH=$(dirname $PERMATHINGS_DIR_PATH)
EPHEMERA_DIR_PATH=$ML_PROJECT_DIR_PATH/ephemera
SHARED_CACHES_DIR_PATH=$EPHEMERA_DIR_PATH/shared_caches
HF_CACHE_FOLDER_PATH=$SHARED_CACHES_DIR_PATH/huggingface

cd $LIBRARIES_DIR_PATH
HF_TOKEN=$(python3 -c "from secretary import get_secret; print(get_secret('HF_TOKEN'))")
cd $THIS_DIR_PATH

docker run -it -p 7862:7860 --platform=linux/amd64 --gpus '"device=1"' \
		-e HUGGING_FACE_HUB_TOKEN=$HF_TOKEN \
        -v $HF_CACHE_FOLDER_PATH:/root/.cache/huggingface \
	registry.hf.space/stabilityai-stable-diffusion-3-medium:latest python app.py

-----new_file-----
 /permathings/scripts/textgen_webui/build_docker.sh:
#!/bin/bash

THIS_DIR_PATH=$(dirname $(realpath $0))
SCRIPTS_DIR_PATH=$(dirname $THIS_DIR_PATH)
PERMATHINGS_DIR_PATH=$(dirname $SCRIPTS_DIR_PATH)
LIBS_DIR_PATH=$PERMATHINGS_DIR_PATH/libs
ML_PROJECT_DIR_PATH=$(dirname $PERMATHINGS_DIR_PATH)
EPHEMERA_DIR_PATH=$ML_PROJECT_DIR_PATH/ephemera
INPUTS_DIR_PATH=$EPHEMERA_DIR_PATH/inputs
OUTPUTS_DIR_PATH=$EPHEMERA_DIR_PATH/outputs
EBOOK_ANALYSES_DIR_PATH=$OUTPUTS_DIR_PATH/ebook_analyses
INPUT_EBOOKS_DIR_PATH=$INPUTS_DIR_PATH/ebooks
SHARED_CACHES_DIR_PATH=$EPHEMERA_DIR_PATH/shared_caches
SECRETS_FILE_PATH=$EPHEMERA_DIR_PATH/secrets.json

HWID=$(hwid | cut -d ' ' -f 2)

cd $LIBS_DIR_PATH
HF_TOKEN=$(python3 -c "from secretary import get_secret; print(get_secret('HF_TOKEN'))")
cd $THIS_DIR_PATH

# This script builds the vllm_server docker image
TEXT_GET_WEBUI_CACHE_FOLDER_PATH=$SHARED_CACHES_DIR_PATH/tgwui
if [ ! -d $TEXT_GET_WEBUI_CACHE_FOLDER_PATH ]; then
    mkdir -p $TEXT_GET_WEBUI_CACHE_FOLDER_PATH
fi

cd $TEXT_GET_WEBUI_CACHE_FOLDER_PATH

#if "vllm_server" dir is not present, create it
if [ ! -d "tgwui_docker" ]; then
    mkdir -p tgwui_docker
fi
cd tgwui_docker

#remove text-generation-webui if it exists
#if [ -d "text-generation-webui-docker" ]; then
#    rm -rf text-generation-webui-docker
#fi

#if vllm dir is not present, clone it
if [ ! -d "text-generation-webui" ]; then
    git clone https://github.com/Atinoda/text-generation-webui-docker
fi
cd text-generation-webui-docker

mv docker-compose.yml docker-compose.yml.bak
mv docker-compose.build.yml docker-compose.yml

sed -i 's/        - VERSION_TAG="v1.5"/        - VERSION_TAG="nightly"/g' docker-compose.yml
sed -i 's/      target: default/      target: default-nvidia/g' docker-compose.yml

#sed -i 's/                device_ids: \['0'\]/                device_ids: \['0', '1'\]/g' docker-compose.yml
#HF_TOKEN=$(jq -r '.HF_TOKEN' $SECRETS_FILE_PATH) docker compose up --build
#run compose build detach with secrets
HF_TOKEN=$HF_TOKEN docker compose up --build -d

-----new_file-----
 /permathings/scripts/textgen_webui/run.sh:
#!/bin/bash

THIS_DIR_PATH=$(dirname $(realpath $0))
SCRIPTS_DIR_PATH=$(dirname $THIS_DIR_PATH)
PERMATHINGS_DIR_PATH=$(dirname $SCRIPTS_DIR_PATH)
LIBS_DIR_PATH=$PERMATHINGS_DIR_PATH/libs
ML_PROJECT_DIR_PATH=$(dirname $PERMATHINGS_DIR_PATH)
EPHEMERA_DIR_PATH=$ML_PROJECT_DIR_PATH/ephemera
INPUTS_DIR_PATH=$EPHEMERA_DIR_PATH/inputs
OUTPUTS_DIR_PATH=$EPHEMERA_DIR_PATH/outputs
EBOOK_ANALYSES_DIR_PATH=$OUTPUTS_DIR_PATH/ebook_analyses
INPUT_EBOOKS_DIR_PATH=$INPUTS_DIR_PATH/ebooks
SHARED_CACHES_DIR_PATH=$EPHEMERA_DIR_PATH/shared_caches
SECRETS_FILE_PATH=$EPHEMERA_DIR_PATH/secrets.json

cd $LIBS_DIR_PATH
HF_TOKEN=$(python3 -c "from secretary import get_secret; print(get_secret('HF_TOKEN'))")

TEXT_GEN_WEBUI_FOLDER_PATH=$SHARED_CACHES_DIR_PATH/tgwui/tgwui_docker/text-generation-webui-docker

cd $TEXT_GEN_WEBUI_FOLDER_PATH
HF_TOKEN=$HF_TOKEN docker compose up -d
# --build -d

-----new_file-----
 /permathings/scripts/tformers_template/Dockerfile:
FROM nvidia/cuda:11.8.0-cudnn8-devel-ubuntu20.04
LABEL maintainer="Hugging Face"

ARG DEBIAN_FRONTEND=noninteractive

# Use login shell to read variables from `~/.profile` (to pass dynamic created variables between RUN commands)
SHELL ["sh", "-lc"]

# The following `ARG` are mainly used to specify the versions explicitly & directly in this docker file, and not meant
# to be used as arguments for docker build (so far).

ARG PYTORCH='2.2.1'
# Example: `cu102`, `cu113`, etc.
ARG CUDA='cu118'

RUN apt update
RUN apt install -y git libsndfile1-dev tesseract-ocr espeak-ng python python3-pip ffmpeg
RUN python3 -m pip install --no-cache-dir --upgrade pip

ARG REF=main
RUN git clone https://github.com/huggingface/transformers && cd transformers && git checkout $REF

RUN [ ${#PYTORCH} -gt 0 ] && VERSION='torch=='$PYTORCH'.*' ||  VERSION='torch'; echo "export VERSION='$VERSION'" >> ~/.profile
RUN echo torch=$VERSION
# `torchvision` and `torchaudio` should be installed along with `torch`, especial...[truncated middle 825 characters]...ng
RUN python3 -m pip install --no-cache-dir git+https://github.com/huggingface/optimum@main#egg=optimum

# Add aqlm for quantization testing
RUN python3 -m pip install --no-cache-dir aqlm[gpu]==1.0.2

# Add hqq for quantization testing
RUN python3 -m pip install --no-cache-dir hqq

# For GGUF tests
RUN python3 -m pip install --no-cache-dir gguf

# Add autoawq for quantization testing
# >=v0.2.3 needed for compatibility with torch 2.2.1
RUN python3 -m pip install --no-cache-dir https://github.com/casper-hansen/AutoAWQ/releases/download/v0.2.3/autoawq-0.2.3+cu118-cp38-cp38-linux_x86_64.whl

# Add quanto for quantization testing
RUN python3 -m pip install --no-cache-dir quanto

# Add eetq for quantization testing
RUN python3 -m pip install git+https://github.com/NetEase-FuXi/EETQ.git

# When installing in editable mode, `transformers` is not recognized as a package.
# this line must be added in order for python to be aware of transformers.
RUN cd transformers && python3 setup.py develop


-----new_file-----
 /permathings/scripts/tformers_template/run.sh:
THIS_DOCKER_CONTAINER_NAME="transformers-quantization-latest-gpu"

THIS_DIR_PATH=$(dirname $(realpath $0))
SCRIPTS_DIR_PATH=$(dirname $THIS_DIR_PATH)
PERMATHINGS_DIR_PATH=$(dirname $SCRIPTS_DIR_PATH)
ML_PROJECT_DIR_PATH=$(dirname $PERMATHINGS_DIR_PATH)
EPHEMERA_DIR_PATH=$ML_PROJECT_DIR_PATH/ephemera
SHARED_CACHES_DIR_PATH=$EPHEMERA_DIR_PATH/shared_caches

if [ ! -f ./Dockerfile ]; then
    wget https://raw.githubusercontent.com/huggingface/transformers/main/docker/transformers-quantization-latest-gpu/Dockerfile 
fi

docker build -t $THIS_DOCKER_CONTAINER_NAME .

#if [ ! "$(docker ps -q -f name=$THIS_DOCKER_CONTAINER_NAME)" == "" ]; then
#    docker stop $THIS_DOCKER_CONTAINER_NAME
#fi

#if [ "$(docker ps -aq -f status=exited -f name=$THIS_DOCKER_CONTAINER_NAME)" ]; then
#    docker rm $THIS_DOCKER_CONTAINER_NAME
#fi

if [ ! -d $SHARED_CACHES_DIR_PATH ]; then
    mkdir -p $SHARED_CACHES_DIR_PATH
fi

if [ ! -d $SHARED_CACHES_DIR_PATH/$THIS_DOCKER_CONTAINER_NAME ]; then
    mkdir -p $SHARED_CACHES_DIR_PATH/$THIS_DOCKER_CONTAINER_NAME
fi

WORKDIR_PATH=$THIS_DIR_PATH/workspace

if [ ! -d $WORKDIR_PATH ]; then
    mkdir -p $WORKDIR_PATH
fi

docker run -it --gpus all --rm \
    --name $THIS_DOCKER_CONTAINER_NAME \
    -v $SHARED_CACHES_DIR_PATH/$THIS_DOCKER_CONTAINER_NAME:/root/.cache \
    -v $WORKDIR_PATH:/workspace \
    -w /workspace \
    $THIS_DOCKER_CONTAINER_NAME


-----new_file-----
 /permathings/scripts/tts_webui/build_docker.sh:
#!/bin/bash

THIS_DIR_PATH=$(dirname $(realpath $0))
SCRIPTS_DIR_PATH=$(dirname $THIS_DIR_PATH)
PERMATHINGS_DIR_PATH=$(dirname $SCRIPTS_DIR_PATH)
LIBS_DIR_PATH=$PERMATHINGS_DIR_PATH/libs
ML_PROJECT_DIR_PATH=$(dirname $PERMATHINGS_DIR_PATH)
EPHEMERA_DIR_PATH=$ML_PROJECT_DIR_PATH/ephemera
INPUTS_DIR_PATH=$EPHEMERA_DIR_PATH/inputs
OUTPUTS_DIR_PATH=$EPHEMERA_DIR_PATH/outputs
EBOOK_ANALYSES_DIR_PATH=$OUTPUTS_DIR_PATH/ebook_analyses
INPUT_EBOOKS_DIR_PATH=$INPUTS_DIR_PATH/ebooks
SHARED_CACHES_DIR_PATH=$EPHEMERA_DIR_PATH/shared_caches
SECRETS_FILE_PATH=$EPHEMERA_DIR_PATH/secrets.json

HWID=$(hwid | cut -d ' ' -f 2)

cd $LIBS_DIR_PATH
HF_TOKEN=$(python3 -c "from secretary import get_secret; print(get_secret('HF_TOKEN'))")
cd $THIS_DIR_PATH

# This script builds the vllm_server docker image
TEXT_GET_WEBUI_CACHE_FOLDER_PATH=$SHARED_CACHES_DIR_PATH/ttswui
if [ ! -d $TEXT_GET_WEBUI_CACHE_FOLDER_PATH ]; then
    mkdir -p $TEXT_GET_WEBUI_CACHE_FOLDER_PATH
fi

cd $TEXT_GET_WEBUI_CACHE_FOLDER_PATH

#remove text-generation-webui if it exists
#if [ -d "text-generation-webui-docker" ]; then
#    rm -rf text-generation-webui-docker
#fi

if [ ! -d "tts-generation-webui" ]; then
    git clone https://github.com/rsxdalv/tts-generation-webui
fi
cd tts-generation-webui

#mv docker-compose.yml docker-compose.yml.bak
#mv docker-compose.build.yml docker-compose.yml

#sed -i 's/        - VERSION_TAG="v1.5"/        - VERSION_TAG="nightly"/g' docker-compose.yml
#sed -i 's/      target: default/      target: default-nvidia/g' docker-compose.yml

#sed -i 's/                device_ids: \['0'\]/                device_ids: \['0', '1'\]/g' docker-compose.yml
#HF_TOKEN=$(jq -r '.HF_TOKEN' $SECRETS_FILE_PATH) docker compose up --build
#run compose build detach with secrets
HF_TOKEN=$HF_TOKEN docker build -t rsxdalv/tts-generation-webui .

-----new_file-----
 /permathings/scripts/tts_webui/run.sh:
#!/bin/bash

THIS_DIR_PATH=$(dirname $(realpath $0))
SCRIPTS_DIR_PATH=$(dirname $THIS_DIR_PATH)
PERMATHINGS_DIR_PATH=$(dirname $SCRIPTS_DIR_PATH)
LIBS_DIR_PATH=$PERMATHINGS_DIR_PATH/libs
ML_PROJECT_DIR_PATH=$(dirname $PERMATHINGS_DIR_PATH)
EPHEMERA_DIR_PATH=$ML_PROJECT_DIR_PATH/ephemera
INPUTS_DIR_PATH=$EPHEMERA_DIR_PATH/inputs
OUTPUTS_DIR_PATH=$EPHEMERA_DIR_PATH/outputs
EBOOK_ANALYSES_DIR_PATH=$OUTPUTS_DIR_PATH/ebook_analyses
INPUT_EBOOKS_DIR_PATH=$INPUTS_DIR_PATH/ebooks
SHARED_CACHES_DIR_PATH=$EPHEMERA_DIR_PATH/shared_caches
SECRETS_FILE_PATH=$EPHEMERA_DIR_PATH/secrets.json

cd $LIBS_DIR_PATH
HF_TOKEN=$(python3 -c "from secretary import get_secret; print(get_secret('HF_TOKEN'))")

TEXT_GEN_WEBUI_FOLDER_PATH=$SHARED_CACHES_DIR_PATH/ttswui/tts-generation-webui

cd $TEXT_GEN_WEBUI_FOLDER_PATH
HF_TOKEN=$HF_TOKEN docker compose up -d
# --build -d

-----new_file-----
 /permathings/scripts/utils/digest_git.py:
import argparse
import os
import tempfile
import zipfile
from urllib.parse import urlparse
import requests
import logging
from math import floor

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def download_repo(url):
    if url.endswith('.git'):
        url = url[:-4]
    if not url.endswith('/archive/main.zip'):
        url += '/archive/main.zip'  # Try 'main' branch
    
    logger.info(f"Attempting to download from URL: {url}")
    response = requests.get(url)
    if response.status_code != 200:
        # If 'main' branch doesn't exist, try 'master'
        url = url.replace('/main.zip', '/master.zip')
        logger.info(f"Main branch not found. Trying master: {url}")
        response = requests.get(url)
        if response.status_code != 200:
            raise ValueError(f"Failed to download repository: HTTP status code {response.status_code}")
    
    with tempfile.NamedTemporaryFile(delete=False, suffix='.zip') as temp_file:
        temp_file.writ...[truncated middle 6462 characters]...ude per file")
    parser.add_argument("--demarcation_string", default="\n-----new_file-----\n", help="String to use between files")
    parser.add_argument("--ignored_extensions", nargs="+", help="File extensions to ignore")
    parser.add_argument("--whitelisted_extensions", nargs="+", help="File extensions to include (if specified, only these will be processed)")
    parser.add_argument("--depth_augmented_limit_per_folder", type=int, default=40, help="Starting limit for files per folder, halved with each level of depth")
    parser.add_argument("--max_folder_depth", type=int, default=8, help="Maximum folder depth to process")
    parser.add_argument("--lower_bound_limit_per_folder", type=int, default=2, help="Minimum number of files to show per folder")
    
    args = parser.parse_args()
    
    main(args.url_or_path, args.max_size_per_file, args.demarcation_string, args.ignored_extensions, args.whitelisted_extensions, args.depth_augmented_limit_per_folder, args.max_folder_depth)


-----new_file-----
 /permathings/scripts/utils/init_secrets.sh:
THIS_DIR_PATH=$(dirname $(realpath $0))
SCRIPTS_DIR_PATH=$(dirname $THIS_DIR_PATH)
PERMATHINGS_DIR_PATH=$(dirname $SCRIPTS_DIR_PATH)
LIBRARIES_DIR_PATH=$PERMATHINGS_DIR_PATH/libs
ML_PROJECT_DIR_PATH=$(dirname $PERMATHINGS_DIR_PATH)
EPHEMERA_DIR_PATH=$ML_PROJECT_DIR_PATH/ephemera

if [ ! -d $EPHEMERA_DIR_PATH ]; then
    mkdir $EPHEMERA_DIR_PATH
fi

cd $LIBRARIES_DIR_PATH
python3 -c "from secretary import get_all_defaults; get_all_defaults()"
cd $THIS_DIR_PATH


-----new_file-----
 /permathings/scripts/utils/set_power.sh:
sudo nvidia-smi -pl 220

-----new_file-----
 /permathings/scripts/webscraping/run.py:
import os, sys, json, re, time, datetime, random, string, docker, atexit

this_dir = os.path.dirname(os.path.abspath(__file__))
scripts_dir = os.path.dirname(this_dir)
permathings_dir = os.path.dirname(scripts_dir)
root_dir = os.path.dirname(permathings_dir)
ephemeral_dir = os.path.join(root_dir, "ephemera")
outputs_dir = os.path.join(ephemeral_dir, "outputs")
libs_dir = os.path.join(permathings_dir, "libs")
sys.path.append(libs_dir)

from selenium_tools import test_tools

test_tools()

